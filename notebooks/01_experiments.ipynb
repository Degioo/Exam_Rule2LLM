{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Imports\n",
                "Import necessary libraries and project modules. Set up paths to access `src` code."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, json\n",
                "from pathlib import Path\n",
                "import pandas as pd\n",
                "\n",
                "# Make src importable\n",
                "import sys\n",
                "sys.path.append(str(Path(\"..\").resolve()))  # if notebook inside /notebooks\n",
                "sys.path.append(str((Path(\"..\") / \"src\").resolve()))\n",
                "\n",
                "from src.common.io import load_docs, load_rules, load_gold\n",
                "from src.baselines.tfidf_ir import tfidf_ir_predict\n",
                "from src.baselines.logistic_baseline import logistic_predict\n",
                "from src.baselines.fasttext_05a import fasttext_predict_05A\n",
                "from src.eval.metrics import evaluate"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Data\n",
                "Load documents, compliance rules, and gold standard labels from the `data` directory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded docs: ['doc_001', 'doc_002', 'doc_003', 'DOC_004', 'DOC_005', 'DOC_006', 'DOC_007', 'DOC_008', 'DOC_009', 'DOC_010', 'DOC_011', 'DOC_012', 'DOC_013', 'DOC_014', 'DOC_015']\n",
                        "Loaded rules: ['01', '02', '03', '05A', '08', '10A', '12', '13']\n"
                    ]
                }
            ],
            "source": [
                "DOCS_DIR = \"../data/docs\"\n",
                "RULES_PATH = \"../data/rules.json\"\n",
                "GOLD_PATH = \"../data/gold_labels.json\"\n",
                "\n",
                "docs = load_docs(DOCS_DIR)\n",
                "rules = load_rules(RULES_PATH)\n",
                "gold = load_gold(GOLD_PATH)\n",
                "\n",
                "print(\"Loaded docs:\", [d.doc_id for d in docs])\n",
                "print(\"Loaded rules:\", [r.id for r in rules])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Baseline: TF-IDF Information Retrieval\n",
                "Run a simple vector space model baseline. It computes Cosine Similarity between the document text and the rule description. If similarity < threshold, it predicts VIOLATED."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "({'macro_precision': 0.23958333333333334,\n",
                            "  'macro_recall': 0.75,\n",
                            "  'macro_f1': 0.3374295467174724},\n",
                            " [('01',\n",
                            "   {'tp': 3,\n",
                            "    'fp': 9,\n",
                            "    'fn': 0,\n",
                            "    'precision': 0.25,\n",
                            "    'recall': 1.0,\n",
                            "    'f1': 0.4,\n",
                            "    'coverage': 1.0}),\n",
                            "  ('02',\n",
                            "   {'tp': 0,\n",
                            "    'fp': 12,\n",
                            "    'fn': 0,\n",
                            "    'precision': 0.0,\n",
                            "    'recall': 0.0,\n",
                            "    'f1': 0.0,\n",
                            "    'coverage': 1.0})])"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "preds_tfidf = tfidf_ir_predict(\n",
                "    docs=docs,\n",
                "    rules=rules,\n",
                "    model_name=\"TFIDF-IR\",\n",
                "    threshold=0.10  # you can tune this\n",
                ")\n",
                "\n",
                "eval_tfidf = evaluate(preds_tfidf, gold)\n",
                "eval_tfidf[\"overall\"], list(eval_tfidf[\"per_rule\"].items())[:2]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Baseline: Logistic Regression\n",
                "Train a Logistic Regression classifier on TF-IDF features. Note: This is purely illustrative due to the very small dataset size."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "({'macro_precision': 0.25, 'macro_recall': 0.25, 'macro_f1': 0.25},\n",
                            " [('01',\n",
                            "   {'tp': 0,\n",
                            "    'fp': 0,\n",
                            "    'fn': 3,\n",
                            "    'precision': 0.0,\n",
                            "    'recall': 0.0,\n",
                            "    'f1': 0.0,\n",
                            "    'coverage': 1.0}),\n",
                            "  ('02',\n",
                            "   {'tp': 0,\n",
                            "    'fp': 0,\n",
                            "    'fn': 0,\n",
                            "    'precision': 0.0,\n",
                            "    'recall': 0.0,\n",
                            "    'f1': 0.0,\n",
                            "    'coverage': 1.0})])"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "preds_logreg = logistic_predict(\n",
                "    docs=docs,\n",
                "    rules=rules,\n",
                "    gold=gold,\n",
                "    model_name=\"TFIDF+LogReg\"\n",
                ")\n",
                "\n",
                "eval_logreg = evaluate(preds_logreg, gold)\n",
                "eval_logreg[\"overall\"], list(eval_logreg[\"per_rule\"].items())[:2]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Compare Baseline Results\n",
                "Aggregate evaluation metrics (Precision, Recall, F1) for both baselines into a Pandas DataFrame for easy comparison."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "def eval_to_df(eval_obj, model_name):\n",
                "    rows = []\n",
                "    for rid, m in eval_obj[\"per_rule\"].items():\n",
                "        rows.append({\n",
                "            \"model\": model_name,\n",
                "            \"rule\": rid,\n",
                "            \"precision\": m[\"precision\"],\n",
                "            \"recall\": m[\"recall\"],\n",
                "            \"f1\": m[\"f1\"],\n",
                "            \"coverage\": m[\"coverage\"]\n",
                "        })\n",
                "    return pd.DataFrame(rows)\n",
                "\n",
                "df = pd.concat([\n",
                "    eval_to_df(eval_tfidf, \"TFIDF-IR\"),\n",
                "    eval_to_df(eval_logreg, \"TFIDF+LogReg\"),\n",
                "], ignore_index=True)\n",
                "\n",
                "df.sort_values([\"rule\", \"model\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Save Baseline Results\n",
                "Save the summary table to a CSV file in the `results` directory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Saved: ..\\results\\summary.csv\n"
                    ]
                }
            ],
            "source": [
                "OUT_DIR = Path(\"../results\")\n",
                "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "df.to_csv(OUT_DIR / \"summary.csv\", index=False)\n",
                "print(\"Saved:\", OUT_DIR / \"summary.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Baseline: FastText Embeddings (Rule 05A)\n",
                "Run a semantic similarity baseline specifically for Rule 05A using pre-trained Italian FastText embeddings. This approach matches keywords like 'resistente' or 'refrattario', but may fail on negations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "exists: True\n",
                        "absolute: C:\\Users\\loren\\Desktop\\Uni\\Statale\\NLP\\Exam_Rule2LLM\\models\\cc.it.300.bin\n",
                        "suffix: .bin\n",
                        "size MB: 6903.07\n"
                    ]
                }
            ],
            "source": [
                "from pathlib import Path\n",
                "\n",
                "p = Path(\"../models/cc.it.300.bin\")\n",
                "print(\"exists:\", p.exists())\n",
                "print(\"absolute:\", p.resolve())\n",
                "print(\"suffix:\", p.suffix)\n",
                "print(\"size MB:\", round(p.stat().st_size / (1024*1024), 2) if p.exists() else None)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading vectors from ../models/cc.it.300.bin...\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "({'macro_precision': 0.75,\n",
                            "  'macro_recall': 0.6,\n",
                            "  'macro_f1': 0.6666666666666665},\n",
                            " {'tp': 3,\n",
                            "  'fp': 1,\n",
                            "  'fn': 2,\n",
                            "  'precision': 0.75,\n",
                            "  'recall': 0.6,\n",
                            "  'f1': 0.6666666666666665,\n",
                            "  'coverage': 1.0})"
                        ]
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "ft_preds = fasttext_predict_05A(\n",
                "    docs=docs,\n",
                "    model_path=\"../models/cc.it.300.bin\",\n",
                "    threshold=0.60\n",
                ")\n",
                "\n",
                "eval_ft = evaluate(ft_preds, gold)\n",
                "eval_ft[\"overall\"], eval_ft[\"per_rule\"].get(\"05A\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. FastText Results\n",
                "View the performance of the FastText baseline on Rule 05A."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_ft = eval_to_df(eval_ft, \"fastText-sim-05A\")\n",
                "df_ft"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 9. LLM Evaluation Results\n",
                "\n",
                "Load pre-computed predictions produced by `src/llm/runner.py` and evaluated by `src/eval/eval_llm.py`.\n",
                "\n",
                "To reproduce (PowerShell):\n",
                "```powershell\n",
                "# OpenAI\n",
                "python -m src.llm.runner --provider openai --model gpt-4.1 --docs data/docs --rules data/rules.json --out results/predictions/openai_gpt41.jsonl\n",
                "python -m src.eval.eval_llm --predictions results/predictions/openai_gpt41.jsonl --gold data/gold_labels.json --out-dir results/metrics --model-tag openai_gpt41\n",
                "\n",
                "# Ollama\n",
                "python -m src.llm.runner --provider ollama --model mistral:latest --docs data/docs --rules data/rules.json --out results/predictions/ollama_mistral.jsonl\n",
                "python -m src.eval.eval_llm --predictions results/predictions/ollama_mistral.jsonl --gold data/gold_labels.json --out-dir results/metrics --model-tag ollama_mistral\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "openai_gpt41 : macro_f1=0.6332  precision=0.6125  recall=0.7292  abstention=0.00%  n=120\n",
                        "ollama_mistral: macro_f1=0.0369  precision=0.0645  recall=0.0296  abstention=8.94%  n=123\n"
                    ]
                }
            ],
            "source": [
                "import json\n",
                "from pathlib import Path\n",
                "import pandas as pd\n",
                "\n",
                "METRICS_DIR = Path(\"../results/metrics\")\n",
                "\n",
                "# Load overall CSVs for both models\n",
                "llm_models = [\"openai_gpt41\", \"ollama_mistral\"]\n",
                "overall_frames = []\n",
                "for tag in llm_models:\n",
                "    p = METRICS_DIR / f\"{tag}_overall.csv\"\n",
                "    if p.exists():\n",
                "        overall_frames.append(pd.read_csv(p))\n",
                "\n",
                "df_llm_overall = pd.concat(overall_frames, ignore_index=True)\n",
                "\n",
                "for _, row in df_llm_overall.iterrows():\n",
                "    print(f\"{row['model']:<20}: macro_f1={row['macro_f1']:.4f}  \"\n",
                "          f\"precision={row['macro_precision']:.4f}  recall={row['macro_recall']:.4f}  \"\n",
                "          f\"abstention={row['abstention_rate']:.2%}  n={int(row['n_predictions'])}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. LLM Per-Rule Breakdown\n",
                "\n",
                "Load per-rule CSV for the best LLM model (OpenAI gpt-4.1)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "   model rule_id  tp  fp  fn  precision    recall        f1  coverage\n",
                            "0  openai_gpt41      01   3   0   0       1.00    1.0000    1.0000       1.0\n",
                            "1  openai_gpt41      02   0   2   0       0.00    0.0000    0.0000       1.0\n",
                            "2  openai_gpt41      03   1   4   0       0.20    1.0000    0.3333       1.0\n",
                            "3  openai_gpt41     05A   7   3   0       0.70    1.0000    0.8235       1.0\n",
                            "4  openai_gpt41      08   2   0   0       1.00    1.0000    1.0000       1.0\n",
                            "5  openai_gpt41     10A   8   0   0       1.00    1.0000    1.0000       1.0\n",
                            "6  openai_gpt41      12   5   0   1       1.00    0.8333    0.9091       1.0\n",
                            "7  openai_gpt41      13   0   3   0       0.00    0.0000    0.0000       1.0\n"
                        ]
                    },
                    "execution_count": 20,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df_gpt41_per_rule = pd.read_csv(METRICS_DIR / \"openai_gpt41_per_rule.csv\")\n",
                "df_gpt41_per_rule"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Full Comparison — All Models\n",
                "\n",
                "Combine baselines and LLM results into a single summary table.\n",
                "\n",
                "> **Note on Ollama/Mistral:** Mistral returned hallucinated rule IDs (`R1`–`R11`, `rule_1`–`rule_12`) instead of the canonical IDs specified in the prompt. This instruction-following failure drove its F1 to near-zero and is discussed in the paper as an error analysis finding."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "                     model  macro_precision  macro_recall  macro_f1  abstention_rate\n",
                            "0                 TFIDF-IR           0.2396        0.7500    0.3374              0.0\n",
                            "1             TFIDF+LogReg           0.2500        0.2500    0.2500              0.0\n",
                            "2         fastText-sim-05A           0.7500        0.6000    0.6667              0.0\n",
                            "3             openai_gpt41           0.6125        0.7292    0.6332              0.0\n",
                            "4           ollama_mistral            0.0645        0.0296    0.0369             0.0894\n"
                        ]
                    },
                    "execution_count": 21,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Load master results CSV\n",
                "df_all = pd.read_csv(Path(\"../results/run_results.csv\"))\n",
                "\n",
                "# Display clean summary\n",
                "df_summary = df_all[[\n",
                "    \"model\", \"macro_precision\", \"macro_recall\", \"macro_f1\", \"abstention_rate\"\n",
                "]].copy()\n",
                "df_summary = df_summary.sort_values(\"macro_f1\", ascending=False).reset_index(drop=True)\n",
                "df_summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save full combined summary\n",
                "df_summary.to_csv(Path(\"../results/metrics/summary.csv\"), index=False)\n",
                "print(\"Updated summary.csv saved.\")\n",
                "df_summary"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "rule2llm",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbformat_minor": 4,
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
